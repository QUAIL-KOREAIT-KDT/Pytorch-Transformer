## R. 트랜스포머 확장 및 응용 분석

### R.1 대표적 변형 구조

트랜스포머 아키텍처는 목적에 따라 인코더와 디코더를 분리하거나 변형하여 발전해 왔습니다.

- BERT (Encoder Only)
    - 인코더 스택만을 활용하여 문맥을 양방향(Bidirectional)으로 이해합니다.
    - 이는 특정 시점의 상태를 결정하기 위해 과거와 미래의 모든 위상 정보를 동시에 참조하는 방식이며, 질문 답변(QA)이나 문장 분류 등 상태 판별에 최적화되어 있습니다.

- GPT (Decoder Only)
    - 디코더를 활용하여 다음에 올 단어를 에너지가 낮은(확률이 높은) 상태로 예측하는 자기회귀(Autoregressive) 모델입니다.
    - 이전 상태들로부터 미래 상태를 생성해 나가는 파동 함수 전개와 유사하며, 대화형 AI의 핵심 엔진으로 자리 잡았습니다.

- Vision Transformer (ViT)
    - 이미지를 작은 조각(Patch)으로 나누어 단어와 같은 시퀀스로 취급합니다.
    - 격자 구조의 픽셀 데이터를 전역적 어텐션으로 처리함으로써, 컴퓨터 비전 분야에서도 최첨단(SOTA) 성능을 달성하고 있습니다.
    
### R.2 범용 아키텍처가 된 이유: 낮은 귀납적 편향(Inductive Bias)

트랜스포머는 데이터에 대한 사전 고정관념(귀납적 편향)이 매우 낮습니다.

- 자율적 관계 형성
    - "단어는 반드시 순서대로 온다(RNN)"거나 "이미지는 인접 픽셀끼리만 연관된다(CNN)"는 제약 없이, 어텐션 메커니즘을 통해 데이터 스스로 최적의 상관관계를 찾게 합니다.
    - 도메인 확장성: 이러한 특성 덕분에 텍스트, 이미지, 소리뿐만 아니라 박사님의 연구 분야인 **물리 실험 데이터(시계열 신호)**나 결정 구조 분석 등 다양한 도메인에서 동일한 알고리즘으로 처리할 수 있는 범용성을 제공합니다.

### R.3 트랜스포머의 진화 흐름


| 시대 | 모델 | 주요 특징 |
| :--- | :--- | :--- |
| **초기 (2017)** | **Transformer** | 순환(Recurrence)을 완전히 제거한 혁신 |
| **도약기 (2018-2019)** | **BERT, GPT-2** | 사전 학습(Pre-training)을 통한 전이 학습 대중화 |
| **규모의 시대 (2020-)** | **GPT-3, Llama** | 모델 크기를 키워 인간 수준의 추론 능력 확보 |
| **확장기 (현재)** | **Multi-modal** | 텍스트뿐만 아니라 이미지, 영상, 오디오를 동시에 이해 |


결론적으로, 트랜스포머는 복잡한 물리적 순환 구조 없이도 정보 간의 상관관계(Attention)를 정확히 계산함으로써 언어와 세상을 이해할 수 있음을 증명했습니다. 이는 인공지능이 자연의 근본 원리인 상태의 중첩과 관측을 데이터 처리의 핵심으로 받아들인 결과라고 할 수 있습니다.
