# νΈλμ¤ν¬λ¨Έ λ…Όλ¬Έ κΈ°λ° λ¶„μ„ λ° κµ¬ν„ ν€ κ³Όμ 

## π“‹ ν”„λ΅μ νΈ κ°μ”

Transformer λ…Όλ¬Έ "Attention Is All You Need"λ¥Ό λ¶„μ„ν•κ³ , ν•µμ‹¬ κµ¬μ΅°λ¥Ό PyTorchλ΅ μ§μ ‘ κµ¬ν„ν•μ—¬ IMDB μν™” λ¦¬λ·° κ°μ • λ¶„λ¥ μ‘μ—…μ„ μν–‰ν• ν”„λ΅μ νΈμ…λ‹λ‹¤.

---

## π“ νμΌ κµ¬μ΅°

```
.
β”β”€β”€ README.md                    # ν”„λ΅μ νΈ κ°μ” (λ³Έ λ¬Έμ„)
β”β”€β”€ Transformer_paper.md          # Transformer λ…Όλ¬Έ μ”μ•½ λ° ν•µμ‹¬ μ΄λ΅  μ •λ¦¬
β”β”€β”€ Transformer_PyTorchκµ¬ν„.ipynb          # Transformer λ³€ν• κµ¬μ΅° λ° μ‘μ© λ¶„μ„ (BERT, GPT, ViT)
β”β”€β”€ Transformer_λ―Έλ‹μ‹¤ν—.ipynb               # κΈ°λ³Έ Transformer μ»΄ν¬λ„νΈ κµ¬ν„ λ° κ²€μ¦
β””β”€β”€ Transformer_ν™•μ¥.md     # IMDB κ°μ • λ¶„λ¥ μ „μ²΄ μ‹¤ν— μ½”λ“
```

---

## 1. λ…Όλ¬Έ μ”μ•½

### ν•µμ‹¬ λ‚΄μ©
- **RNN/CNN μ κ±°**: μν™(Recurrence) λ° ν•©μ„±κ³±(Convolution) κµ¬μ΅°λ¥Ό μ™„μ „ν λ°°μ ν•κ³  μ¤μ§ Attention λ©”μ»¤λ‹μ¦λ§ μ‚¬μ©
- **λ³‘λ ¬ν™” κ°€λ¥**: μμ°¨μ  κ³„μ‚° μ μ•½μ„ μ κ±°ν•μ—¬ ν•™μµ μ†λ„ λ€ν­ ν–¥μƒ
- **μ¥κ±°λ¦¬ μμ΅΄μ„± ν•΄κ²°**: λ‹¨μ–΄ κ°„ κ±°λ¦¬μ™€ λ¬΄κ΄€ν•κ² μƒμ μ‹κ°„ λ‚΄ μ „μ—­μ  κ΄€κ³„ νμ•…

### μ£Όμ” κµ¬μ΅°
1. **Positional Encoding**: μ‚Όκ°ν•¨μ κΈ°λ° μ„μΉ μ •λ³΄ μΈμ½”λ”©
2. **Scaled Dot-Product Attention**: Query, Key, Valueλ¥Ό ν™μ©ν• μƒκ΄€κ΄€κ³„ κ³„μ‚°
3. **Multi-Head Attention**: μ—¬λ¬ κ΄€μ μ—μ„ λ³‘λ ¬λ΅ μ •λ³΄ μ¶”μ¶
4. **Encoder-Decoder**: κ° 6κ° λ μ΄μ–΄λ΅ κµ¬μ„±λ μ¤νƒ κµ¬μ΅°

### μ„±λ¥
- WMT 2014 μμ–΄-λ…μΌμ–΄ λ²μ—­: **28.4 BLEU** (κΈ°μ΅΄ λ€λΉ„ 2 BLEU ν–¥μƒ)
- ν•™μµ μ‹κ°„: κΈ°μ΅΄ λ¨λΈ λ€λΉ„ **1/4 μ΄ν•μ λ¦¬μ†μ¤**λ΅ SOTA λ‹¬μ„±

> **μμ„Έν• λ‚΄μ©**: `tranformer_paper.md` μ°Έκ³ 

---

## 2. κµ¬ν„ μ„¤λ…

### κµ¬ν„ν• μ»΄ν¬λ„νΈ

#### 2.1 ν•µμ‹¬ λ¨λ“ (`transformer.py`)
```python
1. CustomPositionalEncoding      # μ„μΉ μΈμ½”λ”© (sin/cos ν•¨μ)
2. CustomAttention               # Scaled Dot-Product Attention
3. CustomMultiHeadAttention      # Multi-Head Attention
4. TransformerSentimentClassifier # κ°„λ‹¨ν• κ°μ • λ¶„λ¥κΈ°
```

#### 2.2 μ „μ²΄ μ‹μ¤ν… (`transformer_λ―Έλ‹μ‹¤ν—.py`)
```python
1. Vocabulary                    # λ‹¨μ–΄-μΈλ±μ¤ λ§¤ν•‘
2. IMDBDataset                   # λ°μ΄ν„°μ…‹ ν΄λμ¤
3. PositionalEncoding            # μ„μΉ μΈμ½”λ”©
4. MultiHeadAttention            # Multi-Head Self-Attention
5. FeedForwardNetwork            # Position-wise FFN
6. EncoderBlock                  # Attention + FFN + Residual + LayerNorm
7. TransformerEncoder            # Encoder Block μ¤νƒ
8. SentimentClassifier           # μµμΆ… λ¶„λ¥ λ¨λΈ
```

### μ£Όμ” μμ‹ κµ¬ν„

**Positional Encoding**:
```
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

**Scaled Dot-Product Attention**:
```
Attention(Q, K, V) = softmax(QK^T / βd_k) V
```

> **μμ„Έν• λ‚΄μ©**: `transformer.py`, `transformer_λ―Έλ‹μ‹¤ν—.py` μ°Έκ³ 

---

## 3. μ‹¤ν— κ²°κ³Ό μ”μ•½

### μ‹¤ν— μ„¤μ •
- **λ°μ΄ν„°μ…‹**: IMDB Movie Reviews (50,000κ°)
- **Task**: μ΄μ§„ κ°μ • λ¶„λ¥ (κΈμ •/λ¶€μ •)
- **Split**: Train 25,000 / Test 25,000

### Baseline λ¨λΈ ν•μ΄νΌνλΌλ―Έν„°
| νλΌλ―Έν„° | κ°’ |
|---------|-----|
| Vocabulary Size | 10,000 |
| Max Sequence Length | 256 |
| d_model | 128 |
| num_heads | 8 |
| num_layers | 3 |
| d_ff | 512 |
| Dropout | 0.1 |
| Learning Rate | 1e-4 |
| Batch Size | 32 |
| Epochs | 5 |

### μµμΆ… μ„±λ¥

| μ§€ν‘ | κ°’ |
|------|-----|
| **Test Accuracy** | **83.24%** |
| Train Accuracy | 84.60% |
| Precision | 83.35% |
| Recall | 83.24% |
| F1-Score | 83.23% |
| Parameters | 1,875,074 |

### ν•μ΄νΌνλΌλ―Έν„° μ‹¤ν—

| λ¨λΈ | d_model | Layers | Epochs | Test Acc |
|------|---------|--------|--------|----------|
| Baseline | 128 | 3 | 5 | **83.24%** β… |
| Large | 256 | 3 | 3 | 82.86% |

**λ°κ²¬μ‚¬ν•­**:
- ν° λ¨λΈμ΄ ν•­μƒ μΆ‹μ€ κ²ƒμ€ μ•„λ‹
- IMDBλ” λΉ„κµμ  λ‹¨μν• νƒμ¤ν¬λ΅ d_model=128λ΅ μ¶©λ¶„
- ν° λ¨λΈμ€ μλ ΄μ„ μ„ν•΄ λ” λ§μ€ epoch ν•„μ”

> **μμ„Έν• λ‚΄μ©**: `transformer_λ―Έλ‹μ‹¤ν—.py` ν•λ‹¨ μµμΆ… μ •λ¦¬ μ„Ήμ… μ°Έκ³ 

---

## π”— ν™•μ¥ λ° μ‘μ©

Transformer μ•„ν‚¤ν…μ²λ” λ‹¤μ–‘ν• λ³€ν•μΌλ΅ λ°μ „:

- **BERT** (Encoder Only): μ–‘λ°©ν–¥ λ¬Έλ§¥ μ΄ν•΄, μ§λ¬Έ λ‹µλ³€/λ¶„λ¥ νƒμ¤ν¬
- **GPT** (Decoder Only): μκΈ°νκ·€ μƒμ„±, λ€ν™”ν• AI
- **Vision Transformer (ViT)**: μ΄λ―Έμ§€λ¥Ό ν¨μΉλ΅ λ¶„ν• ν•μ—¬ μ²λ¦¬

> **μμ„Έν• λ‚΄μ©**: `transformer_ν™•μ¥.py` μ°Έκ³ 

---

